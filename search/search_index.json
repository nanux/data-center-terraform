{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deployment Automation for Atlassian DC on K8s \u00b6 Support disclaimer \u00b6 Supported Products and Platforms This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. Current project limitations listed below: AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. This project can be used for bootstrapping Atlassian DC products in a K8s cluster. This tool will stand-up a Kubernetes cluster and all the required infrastructure. It will also install supported Atlassian DC products into this pre-provisioned cluster using the Data Center Helm Charts . Deployment overview \u00b6 The diagram below provides a high level overview of what a typical deployment will look like for each DC product: Bamboo Confluence Jira Architectural overview for Bamboo Architectural overview for Confluence Architectural overview for Jira Cluster size and cost In the installation process, Cluster Autoscaler is installed in the Kubernetes cluster. You can define the initial cluster size , i.e. the number of EC2 instances providing resources to the EKS cluster. This size is only initial and will be automatically adjusted depending on the workload resource requirements. Multiple deployments to a single cluster Multiple DC products can also be provisioned to the same cluster. See the Configuration guide for more details Deploying a Data Center product \u00b6 Prerequisites - steps for environment setup including installation of 3rd party tooling Configuration - steps for configuring deployment Installation - steps for running a deployment Product support \u00b6 The minimum versions that we support for each product are: Bamboo DC Confluence DC Jira DC 8.1 7.13 8.19 Feedback \u00b6 If you find any issues, raise a ticket . If you have general feedback or question regarding the project, use Atlassian Community Kubernetes space . Contributions \u00b6 Contributions are welcome! Find out how to contribute . License \u00b6 Apache 2.0 licensed, see license file .","title":"Home"},{"location":"#deployment-automation-for-atlassian-dc-on-k8s","text":"","title":"Deployment Automation for Atlassian DC on K8s"},{"location":"#support-disclaimer","text":"Supported Products and Platforms This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. Current project limitations listed below: AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. This project can be used for bootstrapping Atlassian DC products in a K8s cluster. This tool will stand-up a Kubernetes cluster and all the required infrastructure. It will also install supported Atlassian DC products into this pre-provisioned cluster using the Data Center Helm Charts .","title":"Support disclaimer"},{"location":"#deployment-overview","text":"The diagram below provides a high level overview of what a typical deployment will look like for each DC product: Bamboo Confluence Jira Architectural overview for Bamboo Architectural overview for Confluence Architectural overview for Jira Cluster size and cost In the installation process, Cluster Autoscaler is installed in the Kubernetes cluster. You can define the initial cluster size , i.e. the number of EC2 instances providing resources to the EKS cluster. This size is only initial and will be automatically adjusted depending on the workload resource requirements. Multiple deployments to a single cluster Multiple DC products can also be provisioned to the same cluster. See the Configuration guide for more details","title":"Deployment overview"},{"location":"#deploying-a-data-center-product","text":"Prerequisites - steps for environment setup including installation of 3rd party tooling Configuration - steps for configuring deployment Installation - steps for running a deployment","title":"Deploying a Data Center product"},{"location":"#product-support","text":"The minimum versions that we support for each product are: Bamboo DC Confluence DC Jira DC 8.1 7.13 8.19","title":"Product support"},{"location":"#feedback","text":"If you find any issues, raise a ticket . If you have general feedback or question regarding the project, use Atlassian Community Kubernetes space .","title":"Feedback"},{"location":"#contributions","text":"Contributions are welcome! Find out how to contribute .","title":"Contributions"},{"location":"#license","text":"Apache 2.0 licensed, see license file .","title":"License"},{"location":"development/HOW_TO_START/","text":"How to start development \u00b6 A number of tools have been used for building this project. When working on the Data Center Terraform project it's recommended that a dev environment is set up with the same tools. CLI Tooling A number of CLI tools are recommended for working on this project. See the Prerequisites guide for details. Golang Golang is used extensively for testing this project, as such it needs to be installed. Check if Go is already installed by running the following command: go version If Go is not installed, install it by following the official instructions . Pre-commit hook Configure pre-commit hook to maintain quality and consistency of the Terraform scripts. Install pre-commit as follows: brew install pre-commit Then configure pre-commit to use the project .pre-commit-config.yaml git add .pre-commit-config.yaml","title":"How to start development"},{"location":"development/HOW_TO_START/#how-to-start-development","text":"A number of tools have been used for building this project. When working on the Data Center Terraform project it's recommended that a dev environment is set up with the same tools. CLI Tooling A number of CLI tools are recommended for working on this project. See the Prerequisites guide for details. Golang Golang is used extensively for testing this project, as such it needs to be installed. Check if Go is already installed by running the following command: go version If Go is not installed, install it by following the official instructions . Pre-commit hook Configure pre-commit hook to maintain quality and consistency of the Terraform scripts. Install pre-commit as follows: brew install pre-commit Then configure pre-commit to use the project .pre-commit-config.yaml git add .pre-commit-config.yaml","title":"How to start development"},{"location":"development/HOW_TO_TEST/","text":"Testing \u00b6 You can find the tests in the ./unittest and ./e2etest subdirectories under /test . Unit tests \u00b6 The unittest subdirectory includes module-level terraform plan validation tests. It is required to implement the unit tests for each module. Make sure each test case covers default, customised and invalid conditions. End-to-end tests \u00b6 The e2etest subdirectory contains the end-to-end infrastructure and product tests. The tests cover the entire deployment process, including the provisioning of resources into a cloud provider. Each product will have one test function that covers all the states. The test function starts with generating configurations for a test environment. You can modify the configuration variables in the createConfig() function. The provisioning process is as follows: Create AWS resources using Terraform. Create an EKS namespace (product name by default). Clone the Atlassian Helm chart repository and install the specified product using Helm. Once the cluster and product are initialized, bambooHealthTests() function will validate the installation result. Requirements \u00b6 See the How to start development guide for details on how your environment should be setup prior to running tests. The repository also uses Terratest to run the tests. Setting up AWS security credentials \u00b6 Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface . Set credentials to connect to cloud provider. The project looks for ~/.aws . For more details refer to AWS cli-configure-quickstart . Running unit tests To run unit tests, use the following commands: go get -v -t -d ./... && go mod tidy go test ./test/unittest/... -v You can use regex keywords to run specific groups of test cases. For example, you can run only VPC module-related tests with go test ./unittest/... -v -run TestVpc . Running end-to-end tests End-to-end tests take approx. 40\u201360 min. to complete. To run end-to-end tests, use the following commands: export TF_VAR_bamboo_license = '<bamboo-license>' mkdir -p ./test/e2etest/artifacts go get -v -t -d ./... && go mod tidy go test ./test/e2etest -v -timeout 60m -run Installer | tee ./test/e2etest/artifacts/e2etest.log GitHub Actions \u00b6 These unit and end-to-end tests run as part of the GitHub Actions setup for this repo . You can find the configuration file for these actions in .github/workflows within the root level of the project.","title":"Testing"},{"location":"development/HOW_TO_TEST/#testing","text":"You can find the tests in the ./unittest and ./e2etest subdirectories under /test .","title":"Testing"},{"location":"development/HOW_TO_TEST/#unit-tests","text":"The unittest subdirectory includes module-level terraform plan validation tests. It is required to implement the unit tests for each module. Make sure each test case covers default, customised and invalid conditions.","title":"Unit tests"},{"location":"development/HOW_TO_TEST/#end-to-end-tests","text":"The e2etest subdirectory contains the end-to-end infrastructure and product tests. The tests cover the entire deployment process, including the provisioning of resources into a cloud provider. Each product will have one test function that covers all the states. The test function starts with generating configurations for a test environment. You can modify the configuration variables in the createConfig() function. The provisioning process is as follows: Create AWS resources using Terraform. Create an EKS namespace (product name by default). Clone the Atlassian Helm chart repository and install the specified product using Helm. Once the cluster and product are initialized, bambooHealthTests() function will validate the installation result.","title":"End-to-end tests"},{"location":"development/HOW_TO_TEST/#requirements","text":"See the How to start development guide for details on how your environment should be setup prior to running tests. The repository also uses Terratest to run the tests.","title":"Requirements"},{"location":"development/HOW_TO_TEST/#setting-up-aws-security-credentials","text":"Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface . Set credentials to connect to cloud provider. The project looks for ~/.aws . For more details refer to AWS cli-configure-quickstart . Running unit tests To run unit tests, use the following commands: go get -v -t -d ./... && go mod tidy go test ./test/unittest/... -v You can use regex keywords to run specific groups of test cases. For example, you can run only VPC module-related tests with go test ./unittest/... -v -run TestVpc . Running end-to-end tests End-to-end tests take approx. 40\u201360 min. to complete. To run end-to-end tests, use the following commands: export TF_VAR_bamboo_license = '<bamboo-license>' mkdir -p ./test/e2etest/artifacts go get -v -t -d ./... && go mod tidy go test ./test/e2etest -v -timeout 60m -run Installer | tee ./test/e2etest/artifacts/e2etest.log","title":"Setting up AWS security credentials"},{"location":"development/HOW_TO_TEST/#github-actions","text":"These unit and end-to-end tests run as part of the GitHub Actions setup for this repo . You can find the configuration file for these actions in .github/workflows within the root level of the project.","title":"GitHub Actions"},{"location":"troubleshooting/LIMITATIONS/","text":"Limitations \u00b6 Supported Products and Platforms This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. Current project limitations listed below: AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. Product limitations \u00b6 At this time, Bamboo Data Center is the only product with support for Terraform deployment. We're planning to add support for more Atlassian Data Center products in the future. Infrastructure limitations \u00b6 Cloud provider \u00b6 Amazon Web Services (AWS) is the only supported cloud platform. Database \u00b6 PostgreSQL is the defined database engine for the products and cannot be modified in the configuration. However, users can change the database instance type and storage size .","title":"Limitations"},{"location":"troubleshooting/LIMITATIONS/#limitations","text":"Supported Products and Platforms This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. Current project limitations listed below: AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future.","title":"Limitations"},{"location":"troubleshooting/LIMITATIONS/#product-limitations","text":"At this time, Bamboo Data Center is the only product with support for Terraform deployment. We're planning to add support for more Atlassian Data Center products in the future.","title":"Product limitations"},{"location":"troubleshooting/LIMITATIONS/#infrastructure-limitations","text":"","title":"Infrastructure limitations"},{"location":"troubleshooting/LIMITATIONS/#cloud-provider","text":"Amazon Web Services (AWS) is the only supported cloud platform.","title":"Cloud provider"},{"location":"troubleshooting/LIMITATIONS/#database","text":"PostgreSQL is the defined database engine for the products and cannot be modified in the configuration. However, users can change the database instance type and storage size .","title":"Database"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/","text":"Support boundaries \u00b6 This page describes what is within our scope of support for Terraform Data Center deployments. Supported Products and Platforms This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. Current project limitations listed below: AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. Additional information Read our troubleshooting tips . Read about the product and platform limitations .","title":"Support boundaries"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#support-boundaries","text":"This page describes what is within our scope of support for Terraform Data Center deployments. Supported Products and Platforms This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. Current project limitations listed below: AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. Additional information Read our troubleshooting tips . Read about the product and platform limitations .","title":"Support boundaries"},{"location":"troubleshooting/TROUBLESHOOTING/","text":"Troubleshooting tips \u00b6 This guide contains general tips on how to investigate an application deployment that doesn't work correctly. How do I uninstall an environment using a different Terraform configuration file? Symptom If you try to uninstall an environment by using a different configuration file than the one you used to install it or by using a different version of the code, you may encounter some issues during uninstallation. In most cases, Terraform reports that the resource cannot be removed because it's in use. Solution Identify the resource and delete it manually from the AWS console, and then restart the uninstallation process. Make sure to always use the same configuration file that was used to install the environment. How do I deal with persistent volumes that do not get removed as part of the Terraform uninstallation? Symptom Uninstall fails to remove the persistent volume. Error: Persistent volume atlassian-dc-share-home-pv still exists ( Bound ) Error: context deadline exceeded Error: Persistent volume claim atlassian-dc-share-home-pvc still exists with Solution If a pod termination stalls, it will block pvc and pv deletion. To fix this problem we need to terminate product pod first and run uninstall command again. kubectl delete pod <stalled-pod> -n atlassian --force To see the stalled pod name you can run the following command: kubectl get pods -n atlassian How do I deal with suspended AWS Auto Scaling Groups during Terraform uninstallation? Symptom If for any reason Auto Scaling Group gets suspended, AWS does not allow Terraform to delete the node group. In cases like this the uninstall process gets interrupted with the following error: Error: error waiting for EKS Node Group ( atlas-<environment_name>-cluster:appNode ) to delete: unexpected state 'DELETE_FAILED' , wanted target '' . last error: 2 errors occurred: * i-06a4b4afc9e7a76b0: NodeCreationFailure: Instances failed to join the kubernetes cluster * eks-appNode-3ebedddc-2d97-ff10-6c23-4900d1d79599: AutoScalingGroupInvalidConfiguration: Couldn ' t terminate instances in ASG as Terminate process is suspended Solution Delete the reported Auto Scaling Group in AWS console and run uninstall command again. How do I deal with Terraform AWS authentication issues during installation? Symptom The following error is thrown: An error occurred ( ExpiredToken ) when calling the GetCallerIdentity operation: The security token included in the request is expired Solution Terraform cannot deploy resources to AWS if your security token has expired. Renew your token and retry. How do I deal with Terraform state lock acquisition errors? If user interrupts the installation or uninstallation process, Terraform won't be able to unlock resources. In this case, Terraform is unable to acquire state lock in the next attempt. Symptom The following error is thrown: Acquiring state lock. This may take a few moments... Error: Error acquiring the state lock Error message: ConditionalCheckFailedException: The conditional request failed Lock Info: ID: 26f7b9a8-1bef-0674-669b-1d60800dea4d Path: atlassian-data-center-terraform-state-xxxxxxxxxx/bamboo-xxxxxxxxxx/terraform.tfstate Operation: OperationTypeApply Who: xxxxxxxxxx@C02CK0JYMD6V Version: 1 .0.9 Created: 2021 -11-04 00 :50:34.736134 +0000 UTC Info: Solution Forcibly unlock the state by running the following command: terraform force-unlock <ID> Where <ID> is the value that appears in the error message. There are two Terraform locks; one for the infrastructure and another for Terraform state. If you are still experiencing lock issues, change the directory to ./modules/tfstate and retry the same command. How do I deal with state data in S3 that does not have the expected content? If Terraform state is locked and users forcefully unlock it using terraform force-unlock <id> , it may not get a chance to update the Digest value in DynamoDB. This prevents Terraform from reading the state data. Symptom The following error is thrown: Error refreshing state: state data in S3 does not have the expected content. This may be caused by unusually long delays in S3 processing a previous state update. Please wait for a minute or two and try again. If this problem persists, and neither S3 nor DynamoDB are experiencing an outage, you may need to manually verify the remote state and update the Digest value stored in the DynamoDB table to the following value: 531ca9bce76bbe0262f610cfc27bbf0b Solution Open DynamoDB page in AWS console and find the table named atlassian_data_center_<region>_<aws_account_id>_tf_lock in the same region as the cluster. Click on Explore Table Items and find the LockID named <table_name>/<environment_name>/terraform.tfstate-md5 . Click on the item and replace the Digest value with the given value in the error message. How do I deal with Pre-existing state in multiple environment? If you start installing a new environment while you already have an active environment installed before, you should NOT use the pre-existing state. The same scenario when you want to uninstall a non-active environment. What is active environment? Active environment is the latest environment you installed or uninstalled. Tip Answer ' NO ' when you get a similar message during installation or uninstallation: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: Symptom Installation or uninstallation break after you chose to use pre-existing state. Solution Clean up the project before proceed. In root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . terraform init -var-file = <config file> Then re-run the install/uninstall script. How do I deal with Module not installed error during uninstallation? There are some Terraform specific modules that are required when performing an uninstall. These modules are generated by Terraform during the install process and are stored in the .terraform folder. If Terraform cannot find these modules, then it won't be able perform an uninstall of the infrastructure. Symptom Error: Module not installed on main.tf line 7 : 7 : module \"tfstate-bucket\" { This module is not yet installed. Run \"terraform init\" to install all modules required by this configuration. Solution In the root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . cd modules/tfstate terraform init -var-file = <config file> Go back to the root of the project and re-run the uninstall.sh script. How to deal with getting credentials: exec: executable aws failed with exit code 2 error? Symptom After performing an install.sh the following error is encountered: Error: Post \"https://0839E580E6ADB7B784AECE0E152D8AF2.gr7.eu-west-1.eks.amazonaws.com/api/v1/namespaces\" : getting credentials: exec: executable aws failed with exit code 2 with module.base-infrastructure.kubernetes_namespace.products, on modules/common/main.tf line 39 , in resource \"kubernetes_namespace\" \"products\" : 39 : resource \"kubernetes_namespace\" \"products\" { Solution Ensure you are using a version of the aws cli that is at least >= 2 The version can be checked by running: aws --version","title":"Troubleshooting tips"},{"location":"troubleshooting/TROUBLESHOOTING/#troubleshooting-tips","text":"This guide contains general tips on how to investigate an application deployment that doesn't work correctly. How do I uninstall an environment using a different Terraform configuration file? Symptom If you try to uninstall an environment by using a different configuration file than the one you used to install it or by using a different version of the code, you may encounter some issues during uninstallation. In most cases, Terraform reports that the resource cannot be removed because it's in use. Solution Identify the resource and delete it manually from the AWS console, and then restart the uninstallation process. Make sure to always use the same configuration file that was used to install the environment. How do I deal with persistent volumes that do not get removed as part of the Terraform uninstallation? Symptom Uninstall fails to remove the persistent volume. Error: Persistent volume atlassian-dc-share-home-pv still exists ( Bound ) Error: context deadline exceeded Error: Persistent volume claim atlassian-dc-share-home-pvc still exists with Solution If a pod termination stalls, it will block pvc and pv deletion. To fix this problem we need to terminate product pod first and run uninstall command again. kubectl delete pod <stalled-pod> -n atlassian --force To see the stalled pod name you can run the following command: kubectl get pods -n atlassian How do I deal with suspended AWS Auto Scaling Groups during Terraform uninstallation? Symptom If for any reason Auto Scaling Group gets suspended, AWS does not allow Terraform to delete the node group. In cases like this the uninstall process gets interrupted with the following error: Error: error waiting for EKS Node Group ( atlas-<environment_name>-cluster:appNode ) to delete: unexpected state 'DELETE_FAILED' , wanted target '' . last error: 2 errors occurred: * i-06a4b4afc9e7a76b0: NodeCreationFailure: Instances failed to join the kubernetes cluster * eks-appNode-3ebedddc-2d97-ff10-6c23-4900d1d79599: AutoScalingGroupInvalidConfiguration: Couldn ' t terminate instances in ASG as Terminate process is suspended Solution Delete the reported Auto Scaling Group in AWS console and run uninstall command again. How do I deal with Terraform AWS authentication issues during installation? Symptom The following error is thrown: An error occurred ( ExpiredToken ) when calling the GetCallerIdentity operation: The security token included in the request is expired Solution Terraform cannot deploy resources to AWS if your security token has expired. Renew your token and retry. How do I deal with Terraform state lock acquisition errors? If user interrupts the installation or uninstallation process, Terraform won't be able to unlock resources. In this case, Terraform is unable to acquire state lock in the next attempt. Symptom The following error is thrown: Acquiring state lock. This may take a few moments... Error: Error acquiring the state lock Error message: ConditionalCheckFailedException: The conditional request failed Lock Info: ID: 26f7b9a8-1bef-0674-669b-1d60800dea4d Path: atlassian-data-center-terraform-state-xxxxxxxxxx/bamboo-xxxxxxxxxx/terraform.tfstate Operation: OperationTypeApply Who: xxxxxxxxxx@C02CK0JYMD6V Version: 1 .0.9 Created: 2021 -11-04 00 :50:34.736134 +0000 UTC Info: Solution Forcibly unlock the state by running the following command: terraform force-unlock <ID> Where <ID> is the value that appears in the error message. There are two Terraform locks; one for the infrastructure and another for Terraform state. If you are still experiencing lock issues, change the directory to ./modules/tfstate and retry the same command. How do I deal with state data in S3 that does not have the expected content? If Terraform state is locked and users forcefully unlock it using terraform force-unlock <id> , it may not get a chance to update the Digest value in DynamoDB. This prevents Terraform from reading the state data. Symptom The following error is thrown: Error refreshing state: state data in S3 does not have the expected content. This may be caused by unusually long delays in S3 processing a previous state update. Please wait for a minute or two and try again. If this problem persists, and neither S3 nor DynamoDB are experiencing an outage, you may need to manually verify the remote state and update the Digest value stored in the DynamoDB table to the following value: 531ca9bce76bbe0262f610cfc27bbf0b Solution Open DynamoDB page in AWS console and find the table named atlassian_data_center_<region>_<aws_account_id>_tf_lock in the same region as the cluster. Click on Explore Table Items and find the LockID named <table_name>/<environment_name>/terraform.tfstate-md5 . Click on the item and replace the Digest value with the given value in the error message. How do I deal with Pre-existing state in multiple environment? If you start installing a new environment while you already have an active environment installed before, you should NOT use the pre-existing state. The same scenario when you want to uninstall a non-active environment. What is active environment? Active environment is the latest environment you installed or uninstalled. Tip Answer ' NO ' when you get a similar message during installation or uninstallation: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: Symptom Installation or uninstallation break after you chose to use pre-existing state. Solution Clean up the project before proceed. In root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . terraform init -var-file = <config file> Then re-run the install/uninstall script. How do I deal with Module not installed error during uninstallation? There are some Terraform specific modules that are required when performing an uninstall. These modules are generated by Terraform during the install process and are stored in the .terraform folder. If Terraform cannot find these modules, then it won't be able perform an uninstall of the infrastructure. Symptom Error: Module not installed on main.tf line 7 : 7 : module \"tfstate-bucket\" { This module is not yet installed. Run \"terraform init\" to install all modules required by this configuration. Solution In the root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . cd modules/tfstate terraform init -var-file = <config file> Go back to the root of the project and re-run the uninstall.sh script. How to deal with getting credentials: exec: executable aws failed with exit code 2 error? Symptom After performing an install.sh the following error is encountered: Error: Post \"https://0839E580E6ADB7B784AECE0E152D8AF2.gr7.eu-west-1.eks.amazonaws.com/api/v1/namespaces\" : getting credentials: exec: executable aws failed with exit code 2 with module.base-infrastructure.kubernetes_namespace.products, on modules/common/main.tf line 39 , in resource \"kubernetes_namespace\" \"products\" : 39 : resource \"kubernetes_namespace\" \"products\" { Solution Ensure you are using a version of the aws cli that is at least >= 2 The version can be checked by running: aws --version","title":"Troubleshooting tips"},{"location":"userguide/CLEANUP/","text":"Uninstallation and Cleanup \u00b6 This guide describes how to uninstall all Atlassian Data Center products and remove cloud environments Do you want to install a DC product but still keep the common infrastructure and other installed products? To uninstall one or more products without destroying the infrastructure, remove the product names from environment's config file and re-run install command. The uninstallation process is destructive The uninstallation process will permanently delete the local volume, shared volume, and the database. Terraform state information can also optionally be removed. Before you begin, make sure that you have an up-to-date backup available in a secure location. The uninstallation script is located in the root folder of the project directory. Usage: ./uninstall.sh [ -t ] [ -c <config.tfvars> ] The following options are available: -t - Delete Terraform state files for all installed environment in the same region using the same AWS account. -c <config_file_path> - Pass a custom configuration file to uninstall the environment provisioned by it. Uninstallation using default and custom configuration files If you used the default configuration file ( config.tfvars ) from the root folder of the project, run the following command: ./uninstall.sh Alternatively if you used a custom configuration file to provision the infrastructure, run the following command using the same configuration file: ./uninstall.sh -c my-custom-config.tfvars Removing Terraform state files \u00b6 We create an AWS S3 bucket and DynamoDB table to store the Terraform state of the environments for each region. Without the state information, Terraform cannot maintain the infrastructure. All environments installed in the same region share one S3 bucket to store the state files. By default, the uninstall script does not remove Terraform state files. Remove Terraform state files only if you confirm there is no other installed environment in the same region. If you have installed multiple environments using the same AWS account in the same region, you need to make sure all those environments are uninstalled before removing terraform state. After deleting the state files, Terraform cannot manage the installed environments . If you have no other environment installed in the same region , you may want to remove the Terraform state files permanently. To remove Terraform state files permanently and delete AWS S3 bucket and DynamoDB, run the uninstallation script with the -t switch: ./uninstall.sh -t -c <config_file_path>","title":"Uninstallation"},{"location":"userguide/CLEANUP/#uninstallation-and-cleanup","text":"This guide describes how to uninstall all Atlassian Data Center products and remove cloud environments Do you want to install a DC product but still keep the common infrastructure and other installed products? To uninstall one or more products without destroying the infrastructure, remove the product names from environment's config file and re-run install command. The uninstallation process is destructive The uninstallation process will permanently delete the local volume, shared volume, and the database. Terraform state information can also optionally be removed. Before you begin, make sure that you have an up-to-date backup available in a secure location. The uninstallation script is located in the root folder of the project directory. Usage: ./uninstall.sh [ -t ] [ -c <config.tfvars> ] The following options are available: -t - Delete Terraform state files for all installed environment in the same region using the same AWS account. -c <config_file_path> - Pass a custom configuration file to uninstall the environment provisioned by it. Uninstallation using default and custom configuration files If you used the default configuration file ( config.tfvars ) from the root folder of the project, run the following command: ./uninstall.sh Alternatively if you used a custom configuration file to provision the infrastructure, run the following command using the same configuration file: ./uninstall.sh -c my-custom-config.tfvars","title":"Uninstallation and Cleanup"},{"location":"userguide/CLEANUP/#removing-terraform-state-files","text":"We create an AWS S3 bucket and DynamoDB table to store the Terraform state of the environments for each region. Without the state information, Terraform cannot maintain the infrastructure. All environments installed in the same region share one S3 bucket to store the state files. By default, the uninstall script does not remove Terraform state files. Remove Terraform state files only if you confirm there is no other installed environment in the same region. If you have installed multiple environments using the same AWS account in the same region, you need to make sure all those environments are uninstalled before removing terraform state. After deleting the state files, Terraform cannot manage the installed environments . If you have no other environment installed in the same region , you may want to remove the Terraform state files permanently. To remove Terraform state files permanently and delete AWS S3 bucket and DynamoDB, run the uninstallation script with the -t switch: ./uninstall.sh -t -c <config_file_path>","title":"Removing Terraform state files"},{"location":"userguide/INSTALLATION/","text":"Installation \u00b6 This guide describes how to provision the cloud environment infrastructure and install Atlassian Data Center products in a Kubernetes cluster running on AWS. Supported Products and Platforms AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. 1. Set up AWS security credentials \u00b6 Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface . 2. Clone the project repository \u00b6 Clone the data-center-terraform project repository from GitHub: git clone -b 1 .0.0 https://github.com/atlassian-labs/data-center-terraform.git && cd data-center-terraform 3. Configure the infrastructure \u00b6 Details of the desired infrastructure to be provisioned can be defined in config.tfvars located in the root level of the cloned project. Additional details on how this file can/should be configured can be found in the Configuration guide . Configuration file location? By default, Terraform uses config.tfvars located in the root level of the project. Can I use a custom configuration file? You can use a custom configuration file, but it must follow the same format as the default configuration file. You can make a copy of config.tfvars , renaming the copy and using config.tfvars as a template to define your own infrastructure configuration. How to install more than one DC product? More than one DC products can be provisioned to the same cluster. See the Configuration guide for more details. You can also install DC products to an existing environment by adding the product in the environment's config file and re-run the install command. Use the same configuration file for uninstallation and cleanup If you have more than one environment, make sure to manage the configuration file for each environment separately. When cleaning up your environment, use the same configuration file that was used to create it originally. 4. Run the installation script \u00b6 Based on how config.tfvars has been configured the installation script will Provision the environment and infrastructure Install the selected DC product(s) Installation is fully automated and requires no user intervention. Terraform is invoked under the hood which handles the creation and management of the Kubernetes infrastructure. Terraform deployment details To keep track of the current state of the resources and manage changes, Terraform creates an AWS S3 bucket to store the current state of the environment. An AWS DynamoDB table is created to handle the locking of remote state files during the installation, upgrade, and cleanup stages to prevent the environment from being modified by more than one process at a time. The installation script, install.sh , is located in the root folder of the project. Usage: ./install.sh [ -c <config_file_path> ] [ -h ] The following options are available: -c <config_file_path> - Pass a custom configuration file when provisioning multiple environments -h - Display help information Using the same cloned repository to manage more than one environment If the repository has already been used to deploy an environment, and that environment is still active, i.e. hasn't been uninstalled yet, deploying a new environment using install.sh will get a prompt with following message: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Previous ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/1-s3.tfstate New ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/2-s3.tfstate Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: This will happen everytime when you switch between different active environments. Since environemnts are independent, answer ' NO ' to continue. If you answered Yes, Terraform will then use the state of active environment to plan and deploy new environment, which will pollute the state of both environments, and end up to an error state. Check troubleshoting guide if you accidentally answered Yes. Running the installation script with no parameters will use the default configuration file to provision the environment. Installation using default and custom configuration files Running the installation script with no parameters will use the default configuration file ( config.tfvars ) to provision the environment: ./install.sh Alternatively a custom configuration file can be specified as follows: ./install.sh -c my-custom-config.tfvars How do I find the service URL of the deployed DC product? When the installation process finishes successfully detailed information about the infrastructure is printed to STDOUT , this includes the product_urls value that can be used to launch the product in the browser. Where do I find the database username and password ? The database master username and password for each product is dynamically generated by Terraform and saved in a Kubernetes secret within the product namespace . To access the database username and password, run the following commands: DB_SECRETS=$(kubectl get secret <product-name>-db-cred -n atlassian -o jsonpath='{.data}') DB_USERNAME=$(echo $DB_SECRETS | jq -r '.username' | base64 --decode) DB_PASSWORD=$(echo $DB_SECRETS | jq -r '.password' | base64 --decode) This saves the decoded username and password to the $DB_USERNAME and $DB_PASSWORD environment variables respectively. Uninstall \u00b6 The deployment and all of its associated resources can be un-installed by following the Uninstallation and cleanup guide.","title":"Installation"},{"location":"userguide/INSTALLATION/#installation","text":"This guide describes how to provision the cloud environment infrastructure and install Atlassian Data Center products in a Kubernetes cluster running on AWS. Supported Products and Platforms AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future.","title":"Installation"},{"location":"userguide/INSTALLATION/#1-set-up-aws-security-credentials","text":"Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface .","title":"1. Set up AWS security credentials"},{"location":"userguide/INSTALLATION/#2-clone-the-project-repository","text":"Clone the data-center-terraform project repository from GitHub: git clone -b 1 .0.0 https://github.com/atlassian-labs/data-center-terraform.git && cd data-center-terraform","title":"2. Clone the project repository"},{"location":"userguide/INSTALLATION/#3-configure-the-infrastructure","text":"Details of the desired infrastructure to be provisioned can be defined in config.tfvars located in the root level of the cloned project. Additional details on how this file can/should be configured can be found in the Configuration guide . Configuration file location? By default, Terraform uses config.tfvars located in the root level of the project. Can I use a custom configuration file? You can use a custom configuration file, but it must follow the same format as the default configuration file. You can make a copy of config.tfvars , renaming the copy and using config.tfvars as a template to define your own infrastructure configuration. How to install more than one DC product? More than one DC products can be provisioned to the same cluster. See the Configuration guide for more details. You can also install DC products to an existing environment by adding the product in the environment's config file and re-run the install command. Use the same configuration file for uninstallation and cleanup If you have more than one environment, make sure to manage the configuration file for each environment separately. When cleaning up your environment, use the same configuration file that was used to create it originally.","title":"3. Configure the infrastructure"},{"location":"userguide/INSTALLATION/#4-run-the-installation-script","text":"Based on how config.tfvars has been configured the installation script will Provision the environment and infrastructure Install the selected DC product(s) Installation is fully automated and requires no user intervention. Terraform is invoked under the hood which handles the creation and management of the Kubernetes infrastructure. Terraform deployment details To keep track of the current state of the resources and manage changes, Terraform creates an AWS S3 bucket to store the current state of the environment. An AWS DynamoDB table is created to handle the locking of remote state files during the installation, upgrade, and cleanup stages to prevent the environment from being modified by more than one process at a time. The installation script, install.sh , is located in the root folder of the project. Usage: ./install.sh [ -c <config_file_path> ] [ -h ] The following options are available: -c <config_file_path> - Pass a custom configuration file when provisioning multiple environments -h - Display help information Using the same cloned repository to manage more than one environment If the repository has already been used to deploy an environment, and that environment is still active, i.e. hasn't been uninstalled yet, deploying a new environment using install.sh will get a prompt with following message: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Previous ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/1-s3.tfstate New ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/2-s3.tfstate Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: This will happen everytime when you switch between different active environments. Since environemnts are independent, answer ' NO ' to continue. If you answered Yes, Terraform will then use the state of active environment to plan and deploy new environment, which will pollute the state of both environments, and end up to an error state. Check troubleshoting guide if you accidentally answered Yes. Running the installation script with no parameters will use the default configuration file to provision the environment. Installation using default and custom configuration files Running the installation script with no parameters will use the default configuration file ( config.tfvars ) to provision the environment: ./install.sh Alternatively a custom configuration file can be specified as follows: ./install.sh -c my-custom-config.tfvars How do I find the service URL of the deployed DC product? When the installation process finishes successfully detailed information about the infrastructure is printed to STDOUT , this includes the product_urls value that can be used to launch the product in the browser. Where do I find the database username and password ? The database master username and password for each product is dynamically generated by Terraform and saved in a Kubernetes secret within the product namespace . To access the database username and password, run the following commands: DB_SECRETS=$(kubectl get secret <product-name>-db-cred -n atlassian -o jsonpath='{.data}') DB_USERNAME=$(echo $DB_SECRETS | jq -r '.username' | base64 --decode) DB_PASSWORD=$(echo $DB_SECRETS | jq -r '.password' | base64 --decode) This saves the decoded username and password to the $DB_USERNAME and $DB_PASSWORD environment variables respectively.","title":"4. Run the installation script"},{"location":"userguide/INSTALLATION/#uninstall","text":"The deployment and all of its associated resources can be un-installed by following the Uninstallation and cleanup guide.","title":"Uninstall"},{"location":"userguide/PREREQUISITES/","text":"Prerequisites \u00b6 Before installing the infrastructure for Atlassian Data Center products, make sure that you meet the following requirements and that your local environment is configured with all the necessary tools. Environment setup \u00b6 Its advised that the tooling below is installed to your development environment. A basic understanding of these tools and their associated concepts is also advisable. Terraform Helm v3.3 or later AWS CLI Kubectl (optional) Kubernetes cluster monitoring tools (optional) Terraform \u00b6 Terraform is an open-source infrastructure as code tool that provides a consistent CLI workflow to create and manage the infrastructure of cloud environments. This project uses Terraform to create and manage the Atlassian Data Center infrastructure on AWS for use with supported Data Center products. Supported Products and Platforms AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. Check if Terraform is already installed by running the following command: terraform version If Terraform is not installed, install it by following the official instructions . Helm \u00b6 Atlassian supports Helm Charts for its Data Center products . This project uses the Data Center Helm charts to package Atlassian DC products as a turnkey solution for your cloud infrastructure. Before using this project, make sure that Helm v3.3 or later is installed on your machine. Check if Helm v3.3 or later is already installed by running the following command: helm version --short If Helm is not installed or you're running a version lower than 3.3, install Helm by following the official instructions . AWS CLI \u00b6 You need to have the AWS CLI tool installed on your local machine before creating the Kubernetes infrastructure. Version 2 Version 2 of the AWS CLI is required. If not using version 2 your experience may vary. Check if AWS CLI version 2 is already installed by running the following command: aws --version If the AWS CLI is not installed or you're running version 1, install AWS CLI version 2 by following the official instructions . Kubectl \u00b6 Kubectl is a command line tool lets you control Kubernetes clusters. Check if kubectl is already installed by running the following command: kubectl version If not installed this can be done by following the official instructions . Kubernetes cluster monitoring tools \u00b6 Kubernetes monitoring and issue diagnosis can be facilitated with a monitoring tool like one of those listed below. Installation and usage of one is not a requirement for deployments with this project but can certainly be of help when problems arise. Kubernetes monitoring tools Prometheus Grafana Weave Scope","title":"Prerequisites"},{"location":"userguide/PREREQUISITES/#prerequisites","text":"Before installing the infrastructure for Atlassian Data Center products, make sure that you meet the following requirements and that your local environment is configured with all the necessary tools.","title":"Prerequisites"},{"location":"userguide/PREREQUISITES/#environment-setup","text":"Its advised that the tooling below is installed to your development environment. A basic understanding of these tools and their associated concepts is also advisable. Terraform Helm v3.3 or later AWS CLI Kubectl (optional) Kubernetes cluster monitoring tools (optional)","title":"Environment setup"},{"location":"userguide/PREREQUISITES/#terraform","text":"Terraform is an open-source infrastructure as code tool that provides a consistent CLI workflow to create and manage the infrastructure of cloud environments. This project uses Terraform to create and manage the Atlassian Data Center infrastructure on AWS for use with supported Data Center products. Supported Products and Platforms AWS is the only supported cloud provider. Bamboo , Confluence , and Jira are the DC products supported by this project. Support for additional DC products will be made available in future. Check if Terraform is already installed by running the following command: terraform version If Terraform is not installed, install it by following the official instructions .","title":" Terraform"},{"location":"userguide/PREREQUISITES/#helm","text":"Atlassian supports Helm Charts for its Data Center products . This project uses the Data Center Helm charts to package Atlassian DC products as a turnkey solution for your cloud infrastructure. Before using this project, make sure that Helm v3.3 or later is installed on your machine. Check if Helm v3.3 or later is already installed by running the following command: helm version --short If Helm is not installed or you're running a version lower than 3.3, install Helm by following the official instructions .","title":" Helm"},{"location":"userguide/PREREQUISITES/#aws-cli","text":"You need to have the AWS CLI tool installed on your local machine before creating the Kubernetes infrastructure. Version 2 Version 2 of the AWS CLI is required. If not using version 2 your experience may vary. Check if AWS CLI version 2 is already installed by running the following command: aws --version If the AWS CLI is not installed or you're running version 1, install AWS CLI version 2 by following the official instructions .","title":" AWS CLI"},{"location":"userguide/PREREQUISITES/#kubectl","text":"Kubectl is a command line tool lets you control Kubernetes clusters. Check if kubectl is already installed by running the following command: kubectl version If not installed this can be done by following the official instructions .","title":" Kubectl"},{"location":"userguide/PREREQUISITES/#kubernetes-cluster-monitoring-tools","text":"Kubernetes monitoring and issue diagnosis can be facilitated with a monitoring tool like one of those listed below. Installation and usage of one is not a requirement for deployments with this project but can certainly be of help when problems arise. Kubernetes monitoring tools Prometheus Grafana Weave Scope","title":" Kubernetes cluster monitoring tools"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/","text":"Bamboo configuration \u00b6 Helm chart version \u00b6 bamboo_helm_chart_version sets the Helm chart version of Bamboo instance. bamboo_helm_chart_version = \"1.2.0\" Bamboo version tag \u00b6 Bamboo will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the bamboo_version_tag to the version you want to install. For more information, see Bamboo Version Tags . bamboo_version_tag = \"<BAMBOO_VERSION_TAG>\" Agent Helm chart version \u00b6 bamboo_helm_chart_version sets the Helm chart version of Bamboo Agent instance. bamboo_agent_helm_chart_version = \"1.2.0\" Agent version tag \u00b6 Bamboo Agent will be installed with the default version defined in its Helm chart . If you want to install a specific version of Agent, you can set the bamboo_agent_version_tag to the version you want to install. For more information, see Bamboo Agent Version Tags . bamboo_agent_version_tag = \"<BAMBOO_AGENT_VERSION_TAG>\" License \u00b6 bamboo_license takes the license key of Bamboo product. Make sure that there is no new lines or spaces in license key. bamboo_license = \"<LICENSE_KEY>\" Sensitive data bamboo_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. System Admin Credentials \u00b6 Four values are required to configure Bamboo system admin credentials. bamboo_admin_username = \"<USERNAME>\" bamboo_admin_password = \"<PASSWORD>\" bamboo_admin_display_name = \"<DISPLAY_NAME>\" bamboo_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bamboo_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Restoring from existing dataset If the dataset_url variable is provided (see Restoring from Backup below), the Bamboo System Admin Credentials properties are ignored. You will need to use user credentials from the dataset to log into the instance. Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bamboo instance. (Used default values as example.) bamboo_cpu = \"1\" bamboo_mem = \"1Gi\" bamboo_min_heap = \"256m\" bamboo_max_heap = \"512m\" Agent instance resource configuration \u00b6 The following variables set number of CPU and amount of memory of Bamboo Agent instances. (Used default values as example.) bamboo_agent_cpu = \"0.25\" bamboo_agent_mem = \"256m\" Number of agents \u00b6 number_of_bamboo_agents sets the number of remote agents to be launched. To disable agents, set this value to 0 . number_of_bamboo_agents = 5 The number of agents is limited to the number of allowed agents in your license. Any agents beyond the allowed number won't be able to join the cluster. A valid license is required to install bamboo agents Bamboo needs a valid license to install remote agents. Disable agents if you don't provide a license at installation time. Database engine version \u00b6 bamboo_db_major_engine_version sets the PostgeSQL engine version that will be used. bamboo_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by Bamboo Database Instance Class \u00b6 bamboo_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bamboo_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 bamboo_db_allocated_storage sets the allocated storage for the database instance in GiB. bamboo_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 bamboo_db_iops sets the requested number of I/O operations per second that the DB instance can support. bamboo_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Restoring from Backup \u00b6 To restore data from an existing Bamboo backup , you can set the dataset_url variable to a publicly accessible URL where the dataset can be downloaded. dataset_url = \"https://bamboo-test-datasets.s3.amazonaws.com/dcapt-bamboo-no-agents.zip\" This dataset is downloaded to the shared home and then imported by the Bamboo instance. To log in to the instance, you will need to use any credentials from the dataset. Provisioning time Restoring from the dataset will increase the time it takes to create the environment.","title":"Bamboo configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#bamboo-configuration","text":"","title":"Bamboo configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#helm-chart-version","text":"bamboo_helm_chart_version sets the Helm chart version of Bamboo instance. bamboo_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#bamboo-version-tag","text":"Bamboo will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the bamboo_version_tag to the version you want to install. For more information, see Bamboo Version Tags . bamboo_version_tag = \"<BAMBOO_VERSION_TAG>\"","title":"Bamboo version tag"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#agent-helm-chart-version","text":"bamboo_helm_chart_version sets the Helm chart version of Bamboo Agent instance. bamboo_agent_helm_chart_version = \"1.2.0\"","title":"Agent Helm chart version"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#agent-version-tag","text":"Bamboo Agent will be installed with the default version defined in its Helm chart . If you want to install a specific version of Agent, you can set the bamboo_agent_version_tag to the version you want to install. For more information, see Bamboo Agent Version Tags . bamboo_agent_version_tag = \"<BAMBOO_AGENT_VERSION_TAG>\"","title":"Agent version tag"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#license","text":"bamboo_license takes the license key of Bamboo product. Make sure that there is no new lines or spaces in license key. bamboo_license = \"<LICENSE_KEY>\" Sensitive data bamboo_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"License"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#system-admin-credentials","text":"Four values are required to configure Bamboo system admin credentials. bamboo_admin_username = \"<USERNAME>\" bamboo_admin_password = \"<PASSWORD>\" bamboo_admin_display_name = \"<DISPLAY_NAME>\" bamboo_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bamboo_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Restoring from existing dataset If the dataset_url variable is provided (see Restoring from Backup below), the Bamboo System Admin Credentials properties are ignored. You will need to use user credentials from the dataset to log into the instance.","title":"System Admin Credentials"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bamboo instance. (Used default values as example.) bamboo_cpu = \"1\" bamboo_mem = \"1Gi\" bamboo_min_heap = \"256m\" bamboo_max_heap = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#agent-instance-resource-configuration","text":"The following variables set number of CPU and amount of memory of Bamboo Agent instances. (Used default values as example.) bamboo_agent_cpu = \"0.25\" bamboo_agent_mem = \"256m\"","title":"Agent instance resource configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#number-of-agents","text":"number_of_bamboo_agents sets the number of remote agents to be launched. To disable agents, set this value to 0 . number_of_bamboo_agents = 5 The number of agents is limited to the number of allowed agents in your license. Any agents beyond the allowed number won't be able to join the cluster. A valid license is required to install bamboo agents Bamboo needs a valid license to install remote agents. Disable agents if you don't provide a license at installation time.","title":"Number of agents"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-engine-version","text":"bamboo_db_major_engine_version sets the PostgeSQL engine version that will be used. bamboo_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by Bamboo","title":"Database engine version"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-instance-class","text":"bamboo_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bamboo_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-allocated-storage","text":"bamboo_db_allocated_storage sets the allocated storage for the database instance in GiB. bamboo_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-iops","text":"bamboo_db_iops sets the requested number of I/O operations per second that the DB instance can support. bamboo_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#restoring-from-backup","text":"To restore data from an existing Bamboo backup , you can set the dataset_url variable to a publicly accessible URL where the dataset can be downloaded. dataset_url = \"https://bamboo-test-datasets.s3.amazonaws.com/dcapt-bamboo-no-agents.zip\" This dataset is downloaded to the shared home and then imported by the Bamboo instance. To log in to the instance, you will need to use any credentials from the dataset. Provisioning time Restoring from the dataset will increase the time it takes to create the environment.","title":"Restoring from Backup"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/","text":"Bitbucket configuration \u00b6 Helm chart version \u00b6 bitbucket_helm_chart_version sets the Helm chart version of Bitbucket instance. Bitbucket_helm_chart_version = \"1.2.0\" Bitbucket version tag \u00b6 Bitbucket will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bitbucket, you can set the bitbucket_version_tag to the version you want to install. For more information, see Bitbucket Version Tags . bitbucket_version_tag = \"<BITBUCKET_VERSION_TAG>\" Number of Bitbucket application nodes \u00b6 bitbucket_replica_count defines the desired number of application nodes. bitbucket_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory. License \u00b6 bitbucket_license takes the license key of Bitbucket product. Make sure that there is no new lines or spaces in license key. bitbucket_license = \"<LICENSE_KEY>\" Sensitive data bitbucket_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. System Admin Credentials \u00b6 Four values are optional to configure Bitbucket system admin credentials. If those values are not provided, then Bitbucket will start in setup page to complete the system admin configuration. bitbucket_admin_username = \"<USERNAME>\" bitbucket_admin_password = \"<PASSWORD>\" bitbucket_admin_display_name = \"<DISPLAY_NAME>\" bitbucket_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bitbucket_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Display Name \u00b6 Set the display name of the Bitbucket instance. Note that this value is only used during installation and changing the value during an upgrade has no effect. bitbucket_display_name = \"<DISPLAY_NAME>\" Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bitbucket instance. (Used default values as example.) bitbucket_cpu = \"1\" bitbucket_mem = \"1Gi\" bitbucket_min_heap = \"256m\" bitbucket_max_heap = \"512m\" Database engine version \u00b6 bitbucket_db_major_engine_version sets the PostgeSQL engine version that will be used. bitbucket_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by Bitbucket Database Instance Class \u00b6 bitbucket_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bitbucket_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 bitbucket_db_allocated_storage sets the allocated storage for the database instance in GiB. bitbucket_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 bitbucket_db_iops sets the requested number of I/O operations per second that the DB instance can support. bitbucket_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . NFS resource configuration \u00b6 The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Bitbucket NFS instance resource configuration bitbucket_nfs_requests_cpu = \"0.25\" bitbucket_nfs_requests_memory = \"256Mi\" bitbucket_nfs_limits_cpu = \"0.25\" bitbucket_nfs_limits_memory = \"256Mi\" Elasticsearch Configuration \u00b6 The following variables set the request for number of CPU, amount of memory, amount of storage, and the number of instances in elasticsearch cluster. (Used default values as example.) # Elasticsearch resource configuration for Bitbucket bitbucket_elasticsearch_cpu = \"0.25\" bitbucket_elasticsearch_mem = \"1Gi\" bitbucket_elasticsearch_storage = 10 bitbucket_elasticsearch_replicas = 2","title":"Bitbucket configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#bitbucket-configuration","text":"","title":"Bitbucket configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#helm-chart-version","text":"bitbucket_helm_chart_version sets the Helm chart version of Bitbucket instance. Bitbucket_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#bitbucket-version-tag","text":"Bitbucket will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bitbucket, you can set the bitbucket_version_tag to the version you want to install. For more information, see Bitbucket Version Tags . bitbucket_version_tag = \"<BITBUCKET_VERSION_TAG>\"","title":"Bitbucket version tag"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#number-of-bitbucket-application-nodes","text":"bitbucket_replica_count defines the desired number of application nodes. bitbucket_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory.","title":"Number of Bitbucket application nodes"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#license","text":"bitbucket_license takes the license key of Bitbucket product. Make sure that there is no new lines or spaces in license key. bitbucket_license = \"<LICENSE_KEY>\" Sensitive data bitbucket_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"License"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#system-admin-credentials","text":"Four values are optional to configure Bitbucket system admin credentials. If those values are not provided, then Bitbucket will start in setup page to complete the system admin configuration. bitbucket_admin_username = \"<USERNAME>\" bitbucket_admin_password = \"<PASSWORD>\" bitbucket_admin_display_name = \"<DISPLAY_NAME>\" bitbucket_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bitbucket_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"System Admin Credentials"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#display-name","text":"Set the display name of the Bitbucket instance. Note that this value is only used during installation and changing the value during an upgrade has no effect. bitbucket_display_name = \"<DISPLAY_NAME>\"","title":"Display Name"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bitbucket instance. (Used default values as example.) bitbucket_cpu = \"1\" bitbucket_mem = \"1Gi\" bitbucket_min_heap = \"256m\" bitbucket_max_heap = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-engine-version","text":"bitbucket_db_major_engine_version sets the PostgeSQL engine version that will be used. bitbucket_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by Bitbucket","title":"Database engine version"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-instance-class","text":"bitbucket_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bitbucket_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-allocated-storage","text":"bitbucket_db_allocated_storage sets the allocated storage for the database instance in GiB. bitbucket_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-iops","text":"bitbucket_db_iops sets the requested number of I/O operations per second that the DB instance can support. bitbucket_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#nfs-resource-configuration","text":"The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Bitbucket NFS instance resource configuration bitbucket_nfs_requests_cpu = \"0.25\" bitbucket_nfs_requests_memory = \"256Mi\" bitbucket_nfs_limits_cpu = \"0.25\" bitbucket_nfs_limits_memory = \"256Mi\"","title":"NFS resource configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#elasticsearch-configuration","text":"The following variables set the request for number of CPU, amount of memory, amount of storage, and the number of instances in elasticsearch cluster. (Used default values as example.) # Elasticsearch resource configuration for Bitbucket bitbucket_elasticsearch_cpu = \"0.25\" bitbucket_elasticsearch_mem = \"1Gi\" bitbucket_elasticsearch_storage = 10 bitbucket_elasticsearch_replicas = 2","title":"Elasticsearch Configuration"},{"location":"userguide/configuration/CONFIGURATION/","text":"Common configuration \u00b6 In order to provision the infrastructure and install an Atlassian Data Center product, you need to create a valid Terraform configuration . All configuration data should go to a Terraform configuration file. The content of the configuration file is divided into two groups: Common configuration Product specific configuration Configuration file format. The configuration file is an ASCII text file with the .tfvars extension. The config file must contain all mandatory configuration items with valid values. If any optional items are missing, the default values will be applied. The mandatory configuration items are those you should define once before the first installation. Mandatory values cannot be changed during the entire environment lifecycle. The optional configuration items are not required for installation by default. Optional values may change at any point in the environment lifecycle. Terraform will retain the latest state of the environment and keep track of any configuration changes made later. The following is an example of a valid configuration file: # Mandatory items environment_name = \"my-bamboo-env\" region = \"us-east-2\" # Optional items resource_tags = { Terraform = \"true\" , Organization = \"atlassian\" , product = \"bamboo\" , } instance_types = [ \"m5.xlarge\" ] desired_capacity = 2 domain = \"mydomain.com\" Common Configuration \u00b6 Environmental properties common to all deployments. Environment Name \u00b6 environment_name provides your environment a unique name within a single cloud provider account. This value cannot be altered after the configuration has been applied. The value will be used to form the name of some resources including VPC and Kubernetes cluster . environment_name = \"<your-environment-name>\" # e.g. \"my-terraform-env\" Format Environment names should start with a letter and can contain letters, numbers, and dashes ( - ). The maximum value length is 24 characters. Region \u00b6 region defines the cloud provider region that the environment will be deployed to. region = \"<REGION>\" # e.g. \"ap-northeast-2\" Format The value must be a valid AWS region . Products \u00b6 The products list can be configured with one or many products. This will result in these products being deployed to the same K8s cluster. For example, if a Jira and Confluence deployment is required this property can be configured as follows: products = [ \"jira\", \"confluence\" ] Product specific infrastructure All of the appropriate infrastructure for the product selection will be stood up by Terraform. Domain \u00b6 We recommend using a domain name to access the application via HTTPS . You will be required to secure a domain name and supply the configuration to the config file. When the domain is provided, Terraform will create a Route53 hosted zone based on the environment name. domain = \"<DOMAIN_NAME>\" # e.g. \"mydomain.com\" A fully qualified domain name uses the following format: <product>.<environment-name>.<domain-name> . For example bamboo.staging.mydomain.com . Ingress controller If a domain name is defined, Terraform will create a nginx-ingress controller in the EKS cluster that will provide access to the application via the domain name. Terraform will also create an ACM certificate to provide secure connections over HTTPS. Provision the infrastructure without a domain When commented out the product will be exposed via an unsecured ( HTTP only) DNS endpoint automatically provisioned as part of the AWS ELB load balancer, for example: http://<load-balancer-id>.<region>.elb.amazonaws.com . This DNS Name will be printed out as part of the outputs after the infrastructure has been provisioned. Resource tags \u00b6 resource_tags are custom metadata for all resources in the environment. You can provide multiple tags as a list. Tag propagation Tag names must be unique, and tags will be propogated to all provisioned resources. resource_tags = { <tag-name- 0 > = \"<tag-value>\" , <tag-name- 1 > = \"<tag-value>\" , ... <tag-name-n> = \"<tag-value>\" , } Using Terraform CLI to apply tags is not recommended and may lead to missing tags in some resources. To apply tags to all resources, follow the installation guide . EKS instance type \u00b6 instance_types defines the instance type for the EKS cluster node group. instance_types = [ \"m5.2xlarge\" ] The instance type must be a valid AWS instance type . Instance type selection The instance type cannot be changed once the infrastructure has been provisioned. Initial EKS node count \u00b6 desired_capacity defines the desired number of nodes that the EKS node group should launch with initially. The default value for the number of nodes in Kubernetes node groups is 1 . Minimum is 1 and maximum is 10 . desired_capacity = <NUMBER_OF_NODES> # between 1 and 10 Cluster size and cost In the installation process, cluster-autoscaler is installed in the Kubernetes cluster. You can define the initial cluster size , i.e. the number of EC2 instances providing resources to the EKS cluster. This size is only initial and will be automatically adjusted depending on the workload resource requirements. Product specific configuration \u00b6 Bamboo Confluence Jira Bamboo specific configuration Confluence specific configuration Jira specific configuration Sensitive Data \u00b6 Sensitive input data will eventually be stored as secrets within Kubernetes cluster . We use config.tfvars file to pass configuration values to Terraform stack. The file itself is plain-text on local machine, and will not be stored in remote backend where all the Terraform state files will be stored encrypted. More info regarding sensitive data in Terraform state can be found here . To avoid storing sensitive data in a plain-text file like config.tfvars , we recommend storing them in environment variables prefixed with TF_VAR_ . Take bamboo_admin_password for example, for Linux-like sytems, run the following command to write bamboo admin password to environment variable: export TF_VAR_bamboo_admin_password = <password> If storing this data as plain-text is not a particular concern for the environment to be deployed, you can also choose to supply the values in config.tfvars file. Uncomment the corresponding line and configure the value there.","title":"Common configuration"},{"location":"userguide/configuration/CONFIGURATION/#common-configuration","text":"In order to provision the infrastructure and install an Atlassian Data Center product, you need to create a valid Terraform configuration . All configuration data should go to a Terraform configuration file. The content of the configuration file is divided into two groups: Common configuration Product specific configuration Configuration file format. The configuration file is an ASCII text file with the .tfvars extension. The config file must contain all mandatory configuration items with valid values. If any optional items are missing, the default values will be applied. The mandatory configuration items are those you should define once before the first installation. Mandatory values cannot be changed during the entire environment lifecycle. The optional configuration items are not required for installation by default. Optional values may change at any point in the environment lifecycle. Terraform will retain the latest state of the environment and keep track of any configuration changes made later. The following is an example of a valid configuration file: # Mandatory items environment_name = \"my-bamboo-env\" region = \"us-east-2\" # Optional items resource_tags = { Terraform = \"true\" , Organization = \"atlassian\" , product = \"bamboo\" , } instance_types = [ \"m5.xlarge\" ] desired_capacity = 2 domain = \"mydomain.com\"","title":"Common configuration"},{"location":"userguide/configuration/CONFIGURATION/#common-configuration_1","text":"Environmental properties common to all deployments.","title":"Common Configuration"},{"location":"userguide/configuration/CONFIGURATION/#environment-name","text":"environment_name provides your environment a unique name within a single cloud provider account. This value cannot be altered after the configuration has been applied. The value will be used to form the name of some resources including VPC and Kubernetes cluster . environment_name = \"<your-environment-name>\" # e.g. \"my-terraform-env\" Format Environment names should start with a letter and can contain letters, numbers, and dashes ( - ). The maximum value length is 24 characters.","title":"Environment Name"},{"location":"userguide/configuration/CONFIGURATION/#region","text":"region defines the cloud provider region that the environment will be deployed to. region = \"<REGION>\" # e.g. \"ap-northeast-2\" Format The value must be a valid AWS region .","title":"Region"},{"location":"userguide/configuration/CONFIGURATION/#products","text":"The products list can be configured with one or many products. This will result in these products being deployed to the same K8s cluster. For example, if a Jira and Confluence deployment is required this property can be configured as follows: products = [ \"jira\", \"confluence\" ] Product specific infrastructure All of the appropriate infrastructure for the product selection will be stood up by Terraform.","title":"Products"},{"location":"userguide/configuration/CONFIGURATION/#domain","text":"We recommend using a domain name to access the application via HTTPS . You will be required to secure a domain name and supply the configuration to the config file. When the domain is provided, Terraform will create a Route53 hosted zone based on the environment name. domain = \"<DOMAIN_NAME>\" # e.g. \"mydomain.com\" A fully qualified domain name uses the following format: <product>.<environment-name>.<domain-name> . For example bamboo.staging.mydomain.com . Ingress controller If a domain name is defined, Terraform will create a nginx-ingress controller in the EKS cluster that will provide access to the application via the domain name. Terraform will also create an ACM certificate to provide secure connections over HTTPS. Provision the infrastructure without a domain When commented out the product will be exposed via an unsecured ( HTTP only) DNS endpoint automatically provisioned as part of the AWS ELB load balancer, for example: http://<load-balancer-id>.<region>.elb.amazonaws.com . This DNS Name will be printed out as part of the outputs after the infrastructure has been provisioned.","title":"Domain"},{"location":"userguide/configuration/CONFIGURATION/#resource-tags","text":"resource_tags are custom metadata for all resources in the environment. You can provide multiple tags as a list. Tag propagation Tag names must be unique, and tags will be propogated to all provisioned resources. resource_tags = { <tag-name- 0 > = \"<tag-value>\" , <tag-name- 1 > = \"<tag-value>\" , ... <tag-name-n> = \"<tag-value>\" , } Using Terraform CLI to apply tags is not recommended and may lead to missing tags in some resources. To apply tags to all resources, follow the installation guide .","title":"Resource tags"},{"location":"userguide/configuration/CONFIGURATION/#eks-instance-type","text":"instance_types defines the instance type for the EKS cluster node group. instance_types = [ \"m5.2xlarge\" ] The instance type must be a valid AWS instance type . Instance type selection The instance type cannot be changed once the infrastructure has been provisioned.","title":"EKS instance type"},{"location":"userguide/configuration/CONFIGURATION/#initial-eks-node-count","text":"desired_capacity defines the desired number of nodes that the EKS node group should launch with initially. The default value for the number of nodes in Kubernetes node groups is 1 . Minimum is 1 and maximum is 10 . desired_capacity = <NUMBER_OF_NODES> # between 1 and 10 Cluster size and cost In the installation process, cluster-autoscaler is installed in the Kubernetes cluster. You can define the initial cluster size , i.e. the number of EC2 instances providing resources to the EKS cluster. This size is only initial and will be automatically adjusted depending on the workload resource requirements.","title":"Initial EKS node count"},{"location":"userguide/configuration/CONFIGURATION/#product-specific-configuration","text":"Bamboo Confluence Jira Bamboo specific configuration Confluence specific configuration Jira specific configuration","title":"Product specific configuration"},{"location":"userguide/configuration/CONFIGURATION/#sensitive-data","text":"Sensitive input data will eventually be stored as secrets within Kubernetes cluster . We use config.tfvars file to pass configuration values to Terraform stack. The file itself is plain-text on local machine, and will not be stored in remote backend where all the Terraform state files will be stored encrypted. More info regarding sensitive data in Terraform state can be found here . To avoid storing sensitive data in a plain-text file like config.tfvars , we recommend storing them in environment variables prefixed with TF_VAR_ . Take bamboo_admin_password for example, for Linux-like sytems, run the following command to write bamboo admin password to environment variable: export TF_VAR_bamboo_admin_password = <password> If storing this data as plain-text is not a particular concern for the environment to be deployed, you can also choose to supply the values in config.tfvars file. Uncomment the corresponding line and configure the value there.","title":"Sensitive Data"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/","text":"Confluence configuration \u00b6 Helm chart version \u00b6 confluence_helm_chart_version sets the Helm chart version of Confluence instance. confluence_helm_chart_version = \"1.2.0\" Confluence version tag \u00b6 Confluence will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the confluence_version_tag to the version you want to install. For more information, see Confluence Version Tags . confluence_version_tag = \"<CONFLUENCE_VERSION_TAG>\" Number of Confluence application nodes \u00b6 The initial Confluence installation require to be started only with a single application node. After all the setup steps are finished, it is possible to update the confluence_replica_count with a number higher than one and run install.sh to update the application node count. # Number of Confluence application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Confluence is fully # installed and configured. confluence_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory. License \u00b6 confluence_license takes the license key of Confluence product. Make sure that there is no new lines or spaces in license key. confluence_license = \"<LICENSE_KEY>\" Sensitive data confluence_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Confluence instance. (Used default values as example.) confluence_cpu = \"2\" confluence_mem = \"1Gi\" confluence_min_heap = \"256m\" confluence_max_heap = \"512m\" Database engine version \u00b6 confluence_db_major_engine_version sets the PostgeSQL engine version that will be used. confluence_db_major_engine_version = \"11\" Supported DB versions Be sure to use a DB engine version that is supported by Confluence Database Instance Class \u00b6 confluence_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . confluence_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 confluence_db_allocated_storage sets the allocated storage for the database instance in GiB. confluence_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 confluence_db_iops sets the requested number of I/O operations per second that the DB instance can support. confluence_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Collaborative editing \u00b6 confluence_collaborative_editing_enabled enables Collaborative editing . (default: true ) confluence_collaborative_editing_enabled = true","title":"Confluence configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#confluence-configuration","text":"","title":"Confluence configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#helm-chart-version","text":"confluence_helm_chart_version sets the Helm chart version of Confluence instance. confluence_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#confluence-version-tag","text":"Confluence will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the confluence_version_tag to the version you want to install. For more information, see Confluence Version Tags . confluence_version_tag = \"<CONFLUENCE_VERSION_TAG>\"","title":"Confluence version tag"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#number-of-confluence-application-nodes","text":"The initial Confluence installation require to be started only with a single application node. After all the setup steps are finished, it is possible to update the confluence_replica_count with a number higher than one and run install.sh to update the application node count. # Number of Confluence application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Confluence is fully # installed and configured. confluence_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory.","title":"Number of Confluence application nodes"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#license","text":"confluence_license takes the license key of Confluence product. Make sure that there is no new lines or spaces in license key. confluence_license = \"<LICENSE_KEY>\" Sensitive data confluence_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"License"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Confluence instance. (Used default values as example.) confluence_cpu = \"2\" confluence_mem = \"1Gi\" confluence_min_heap = \"256m\" confluence_max_heap = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-engine-version","text":"confluence_db_major_engine_version sets the PostgeSQL engine version that will be used. confluence_db_major_engine_version = \"11\" Supported DB versions Be sure to use a DB engine version that is supported by Confluence","title":"Database engine version"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-instance-class","text":"confluence_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . confluence_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-allocated-storage","text":"confluence_db_allocated_storage sets the allocated storage for the database instance in GiB. confluence_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-iops","text":"confluence_db_iops sets the requested number of I/O operations per second that the DB instance can support. confluence_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#collaborative-editing","text":"confluence_collaborative_editing_enabled enables Collaborative editing . (default: true ) confluence_collaborative_editing_enabled = true","title":"Collaborative editing"},{"location":"userguide/configuration/JIRA_CONFIGURATION/","text":"Jira configuration \u00b6 Helm chart version \u00b6 jira_helm_chart_version sets the Helm chart version of Jira instance. jira_helm_chart_version = \"1.2.0\" Jira version tag \u00b6 Jira Software will be installed with the default version defined in its Helm chart . If you want to install a specific version of Jira software, you can set the jira_version_tag to the version you want to install. For more information, see Jira Version Tags . jira_version_tag = \"<JIRA_VERSION_TAG>\" Number of Jira application nodes \u00b6 The initial Jira installation require to be started only with a single application node. After all the setup steps are finished, it is possible to update the jira_replica_count with a number higher than one and run install.sh to update the application node count. # Number of Jira application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Jira is fully # installed and configured. jira_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory. Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Jira instance. (Used default values as example.) jira_cpu = \"1\" jira_mem = \"2Gi\" jira_min_heap = \"384m\" jira_max_heap = \"786m\" jira_reserved_code_cache = \"512m\" Database engine version \u00b6 jira_db_major_engine_version sets the PostgeSQL engine version that will be used. jira_db_major_engine_version = \"12\" Supported DB versions Be sure to use a DB engine version that is supported by Jira Database Instance Class \u00b6 jira_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . jira_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 jira_db_allocated_storage sets the allocated storage for the database instance in GiB. jira_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 jira_db_iops sets the requested number of I/O operations per second that the DB instance can support. jira_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Jira configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#jira-configuration","text":"","title":"Jira configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#helm-chart-version","text":"jira_helm_chart_version sets the Helm chart version of Jira instance. jira_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#jira-version-tag","text":"Jira Software will be installed with the default version defined in its Helm chart . If you want to install a specific version of Jira software, you can set the jira_version_tag to the version you want to install. For more information, see Jira Version Tags . jira_version_tag = \"<JIRA_VERSION_TAG>\"","title":"Jira version tag"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#number-of-jira-application-nodes","text":"The initial Jira installation require to be started only with a single application node. After all the setup steps are finished, it is possible to update the jira_replica_count with a number higher than one and run install.sh to update the application node count. # Number of Jira application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Jira is fully # installed and configured. jira_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory.","title":"Number of Jira application nodes"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Jira instance. (Used default values as example.) jira_cpu = \"1\" jira_mem = \"2Gi\" jira_min_heap = \"384m\" jira_max_heap = \"786m\" jira_reserved_code_cache = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-engine-version","text":"jira_db_major_engine_version sets the PostgeSQL engine version that will be used. jira_db_major_engine_version = \"12\" Supported DB versions Be sure to use a DB engine version that is supported by Jira","title":"Database engine version"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-instance-class","text":"jira_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . jira_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-allocated-storage","text":"jira_db_allocated_storage sets the allocated storage for the database instance in GiB. jira_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-iops","text":"jira_db_iops sets the requested number of I/O operations per second that the DB instance can support. jira_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"}]}